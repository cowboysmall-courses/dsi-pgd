---
title: "Statistical Inference"
author: "Jerry Kiely"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}

library(nortest)
library(gmodels)

knitr::opts_chunk$set(echo = TRUE)

```



\newpage





# Introduction

\vspace{1.5em}





## Basic terms

\vspace{1.5em}

* variable
* population
* sample
* distribution
* factor
* descriptive statistics

\vspace{1.5em}



## What is Statistical Inference?

\vspace{1.5em}

* drawing conclusions about unknown population properties
* based on samples drawn from the population
* such as mean, proportion, variance, etc.
* unknown population properties - known as parameters

\vspace{1.5em}



## Three branches

\vspace{1.5em}

* point estimation
* interval estimation
* testing of hypothesis

\vspace{1.5em}



## Parameter, Estimator, Estimate

\vspace{1.5em}

* parameter - unknown property or characteristic of population
* estimator - rule / function based on sample observations used to estimate parameter
* estimate  - value computed from estimator

\vspace{1.5em}



## Sampling distribution / error

\vspace{1.5em}

* sampling distribution of sample means
* standard error is the standard deviation of the sample means

\vspace{1.5em}



## Hypothesis Testing

\vspace{1.5em}

* hypothesis - assertion of distribution / parameter of one or more random variables
* null hypothesis ($H_{0}$) - assertion believed to be true until rejected
* alternative hypothesis ($H_{1}$) - claim that contradicts $H_{0}$

\vspace{1.5em}

You test a hypothesis to decide if a statement / hypothesis about a population 
parameter is true based on sample data.

\vspace{1.5em}

* test statistic - the statistic on which the decision to reject the null hypothesis is defined
* critical / rejection region - the region within which, if the value of the test statistic falls, the null hypothesis is rejected

\vspace{1.5em}



## Types of Error

\vspace{1.5em}

* Type I Error
* Type II Error

\vspace{1.5em}

|                        | $H_{0}$ is true | $H_{0}$ is false |
|------------------------|-----------------|------------------|
| reject $H_{0}$         | Type I Error    | Correct          |
| fail to reject $H_{0}$ | Correct         | Type II Error    |

\vspace{1.5em}

* level of significance - probability of Type I Error ($\alpha$)
* generally set at $5\%$ or $0.05$
* p-value - smallest level of significance that would lead to rejection of $H_{0}$
* $H_{0}$ rejected if observed risk (or p-value) is less than level of significance
* $\alpha$ = Probability [Type I Error] = Probability [Reject $H_{0}$ | $H_{0}$ is True]
* $\beta$  = Probability [Type II Error] = Probability [Do not reject $H_{0}$ | $H_{0}$ is not True]
* power of the test - $1 - \beta$

\vspace{1.5em}



## One-tailed and two-tailed tests

\vspace{1.5em}

One-tailed test:

$H_{0}: \mu = \mu_{0}$ 

$H_{0}: \mu > \mu_{0}$ (right-tailed) or $H_{0}: \mu < \mu_{0}$ (left-tailed)

\vspace{1.5em}

Two-tailed test:

$H_{0}: \mu = \mu_{0}$ 

$H_{0}: \mu \neq \mu_{0}$

\vspace{1.5em}




\newpage





# Parametric Tests

\vspace{1.5em}





## Normality Tests

\vspace{1.5em}

A prerequisite for many statistical tests - normal data is an underlying assumption in 
parametric tests. Normality can be assessed using two approaches:

* graphical
  * Box-Whisker plot
  * Q-Q plot
* numerical
  * Shapiro-Wilk test (small samples)
  * Kolmogorov-Smirnov test (large samples)

\vspace{1.5em}




### Box-Whisker Plot

\vspace{1.5em}

Powerful visual method for assessing symmetry.

\vspace{1.5em}

```{r data1, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/intro_and_parametric_tests/Normality\ Testing\ Data.csv", header = TRUE)

```

```{r boxplot}

boxplot(data$csi, main = "Box Plot", ylab = "Length", col = "cadetblue")

```

\vspace{1.5em}





### Q-Q Plot

\vspace{1.5em}

Powerful visual method for assessing normality.

\vspace{1.5em}

```{r qqnorm}

qqnorm(data$csi, pch = 1, frame = FALSE)
qqline(data$csi, col = "cadetblue", lwd = 2)

```

\vspace{1.5em}





### Shapiro-Wilk test

\vspace{1.5em}

A widely used test for assessing normality.

\vspace{1.5em}

```{r shapiro}

shapiro.test(data$csi)

```

\vspace{1.5em}




### Kolmogorov-Smirnov test

\vspace{1.5em}

Another widely used test for assessing normality.

\vspace{1.5em}

```{r kolmogorov}

lillie.test(data$csi)

```

\vspace{1.5em}






## t-distribution

\vspace{1.5em}

* symmetric
* resembles bell shape of the normal distribution
* as the sample size increases, as the degrees of freedom increases, it approaches the normal distribution with mean 0 and variance 1

\vspace{1.5em}





## Degrees of freedom

\vspace{1.5em}

* the number of independent terms
* $n$ values would have $n - 1$ degrees of freedom
* $S = x_{1} + x_{2} + x_{3} + x_{4} + x_{5} \implies x_{1} = S - (x_{2} + x_{3} + x_{4} + x_{5})$

\vspace{1.5em}




## One sample t-test

\vspace{1.5em}

* test a hypothesis about a single population mean
* a single sample drawn from a defined population
* compare sample statistic to hypothesized value of a population parameter

\vspace{1.5em}

The assumptions of the one sample t-tests:

\vspace{1.5em}

* random sampling from a defined population
* population is normally distributed
* variable under study is continuous

\vspace{1.5em}

Normality tests can be performed using any of the methods described previously. The 
validity of the test is not significantly affected by moderate deviations from the 
normality assumption.

\vspace{1.5em}





\newpage





\vspace{1.5em}

```{r data2, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/intro_and_parametric_tests/ONE\ SAMPLE\ t\ TEST.csv", header = TRUE)

```

```{r one-sample-t-test}

t.test(data$Time, alternative = "greater", mu = 90)

```

\vspace{1.5em}





## Independent samples t-test

\vspace{1.5em}

* compares means of two independent groups on the same continuous variable
* hypothesis tested
  * $H_{0}$: $\mu_{1} = \mu_{2}$
  * $H_{1}$: $\mu_{1} \neq \mu_{2}$

\vspace{1.5em}

The assumptions of the independent samples t-test:

\vspace{1.5em}

* samples drawn are random samples
* populations from which samples are drawn have equal and unknown variances
* populations follow normal distribution

\vspace{1.5em}

Normality tests can be performed using any of the methods described previously.

\vspace{1.5em}

```{r data3, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/intro_and_parametric_tests/INDEPENDENT\ SAMPLES\ t\ TEST.csv", header = TRUE)

```

```{r independent-samples-t-test-var-equal}

t.test(data$time_g1, data$time_g2, alternative = "two.sided", var.equal = TRUE)

```

\vspace{1.5em}





\newpage





\vspace{1.5em}

```{r independent-samples-t-test-var-unequal}

t.test(data$time_g1, data$time_g2, alternative = "two.sided", var.equal = FALSE)

```

\vspace{1.5em}












## Paired samples t-test

\vspace{1.5em}

* used to determine if the mean difference between two sets of observations is $0$
* each subject is measured twice - paired observations
* typically before / after
* hypothesis tested
  * $H_{0}$: $\mu_{1} - \mu_{2} = 0$
  * $H_{1}$: $\mu_{1} - \mu_{2} \neq 0$

\vspace{1.5em}

The assumptions of the paired sample t-test:

\vspace{1.5em}

* random sampling from a defined population
* population is normally distributed

\vspace{1.5em}

A normality test can be performed using any of the methods described previously. The 
validity of the test is not significantly affected by moderate deviations from the 
normality assumption.

\vspace{1.5em}

```{r data4, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/intro_and_parametric_tests/PAIRED\ t\ TEST.csv", header = TRUE)

```

```{r paired-sample-t-test}

t.test(data$time_before, data$time_after, alternative = "greater", paired = TRUE)

```

\vspace{1.5em}












## t-test for correlation

\vspace{1.5em}

* correlation coefficient summarizes the strength of a linear relationship between two variables
* a t-test is used to test if there is a significant correlation between two variables
* sample correlation coefficient is calculated using bivariate data
* hypothesis tested
  * $H_{0}$: there is no significant correlation between two variables under study ($\rho = 0$)
  * $H_{1}$: there is correlation between two variables under study ($\rho \neq 0$)

\vspace{1.5em}

```{r data5, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/intro_and_parametric_tests/Correlation\ test.csv", header = TRUE)

```

```{r correlation-t-test}

cor.test(data$aptitude, data$job_prof, alternative = "two.sided", method = "pearson")

```

\vspace{1.5em}





\newpage





# Tests for Equality of Variances

\vspace{1.5em}


## F-test

\vspace{1.5em}

* used to test the equality of two population variances
* a prerequisite for many statistical tests
* hypothesis tested
  * $H_{0}$: $\sigma_{1}^{2} = \sigma_{2}^{2}$
  * $H_{1}$: $\sigma_{1}^{2} \neq \sigma_{2}^{2}$

\vspace{1.5em}

The assumptions of the F-test:

\vspace{1.5em}

* random sampling from a defined population
* population is normally distributed

\vspace{1.5em}

The F-test is used to validate the assumption of the equality of variances. The parent 
population is assumed to follow a normal distribution.

\vspace{1.5em}

```{r data6, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/analysis_of_variance/F\ test\ for\ 2\ variances.csv", header = TRUE)

```

```{r f-test}

var.test(data$time_g1, data$time_g2, alternative = "two.sided")

```

\vspace{1.5em}





\newpage





# Analysis of Variance

\vspace{1.5em}

## What is Analysis of Variance

\vspace{1.5em}

* a collection of statistical models used to analyze the difference among more than two group means
* developed by Ronald Fisher
* variance due to
  * assignable causes
  * chance causes
* ANOVA is the separation of variance ascribable to one group of causes from the variance ascribable to another

\vspace{1.5em}

The assumptions of ANOVA:

\vspace{1.5em}

* samples drawn are random samples
* populations from which samples are drawn have equal and unknown variances
* populations follow normal distribution

\vspace{1.5em}

A normality test can be performed using any of the methods described previously.

\vspace{1.5em}

## One Way ANOVA

\vspace{1.5em}

* an extension of the t-test for independent samples
* used to test equality of $K$ population means
  * when $K = 2$ t-Test can be used
  * when $K = 2$ t-Test and one way ANOVA provide identical results
* hypothesis tested
  * $H_{0}$: $\mu_{1} = \mu_{2} = ... = \mu_{K} = \mu$
  * $H_{1}$: $\mu_{i} \neq \mu_{j},  i \neq j$

\vspace{1.5em}

```{r data7, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/analysis_of_variance/One\ way\ anova.csv", header = TRUE)

```

```{r one-way-anova}

anova <- aov(satindex ~ dept, data = data)
summary(anova)

```

\vspace{1.5em}




\newpage





## Two Way ANOVA

\vspace{1.5em}

* used when there are 2 factors under study
* each factor can have 2 or more levels
* three hypothesis tested
  * Factor A
    * $H_{01}$: all group means are equal
    * $H_{11}$: at least one mean is different from other means
  * Factor B
    * $H_{02}$: all group means are equal
    * $H_{12}$: at least one mean is different from other means
  * Interaction
    * $H_{03}$: the interaction is not significant
    * $H_{13}$: the interaction is significant

\vspace{1.5em}

```{r data8, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/analysis_of_variance/Two\ way\ anova.csv", header = TRUE)

```

```{r two-way-anova}

anova <- aov(satindex ~ dept*exp, data = data)
summary(anova)

```

\vspace{1.5em}

## Three Way ANOVA

\vspace{1.5em}

* two way ANOVA can be extended to assess the effects of three or more factors
* with three factors A, B, and C we look at
  * the effects of A, B, and C
  * two way interactions - A\*B, A\*C, and B\*C
  * three way interaction - A\*B\*C

\vspace{1.5em}

```{r data9, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/analysis_of_variance/Three\ Way\ Anova.csv", header = TRUE)

```

```{r three-way-anova}

anova <- aov(growth ~ campaign*region*size, data = data)
summary(anova)

```

\vspace{1.5em}





\newpage





# Non-Parametric Tests

\vspace{1.5em}

* tests based on t and F distributions assume populations are normally distributed
* large body of statistical methods which do not make assumptions about the population distribution
* non-parametric or distribution-free tests
* if underlying normality assumption is met then parametric test should be chosen

\vspace{1.5em}




## Wilcoxon Signed Rank Test

\vspace{1.5em}

* considered a non-parametric alternative to one sample t-test
* used to determine if the mean / median of a sample is equal to a known value when the 
variable is ordinal or continuous, but not normally distributed
* hypothesis tested
  * $H_{0}$: the mean / median of the population is $m_{0}$ 
  * $H_{1}$: the mean / median of the population is not $m_{0}$ 

\vspace{1.5em}

```{r data11, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/non_parametric_tests/Wilcoxon\ Signed\ Rank\ test\ for\ one\ sample.csv", header = TRUE)

```

```{r one-sample-wilcox-test}

wilcox.test(data$Score, mu = 50, alternative = "less")

```

\vspace{1.5em}







## Mann-Whitney Test

\vspace{1.5em}

* considered a non-parametric alternative to independent samples t-test
* used to compare differences between two independent groups when the dependent variable is either ordinal 
or continuous, but not normally distributed
* equivalent to Wilcoxon rank-sum test (WRS)
* hypothesis tested
  * $H_{0}$: the distributions of both groups are identical 
  * $H_{1}$: the distributions of both groups are not identical 

\vspace{1.5em}

```{r data10, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/non_parametric_tests/Mann\ Whitney\ test.csv", header = TRUE)

```

```{r mann-whitney-test}

wilcox.test(aptscore ~ Group, data = data)

```

\vspace{1.5em}










## Wilcoxon Signed Rank Test For Paired Data

\vspace{1.5em}

* considered a non-parametric alternative to paired t-test
* used to compare differences between two related or paired groups when the dependent variable is either 
ordinal or continuous, but not normally distributed
* hypothesis tested
  * $H_{0}$: the median of difference in the population is $0$ 
  * $H_{1}$: not $H_{0}$

\vspace{1.5em}

```{r data12, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/non_parametric_tests/Wilcoxon\ Signed\ Rank\ test\ for\ paired\ data.csv", header = TRUE)

```

```{r paired-data-wilcox-test}

wilcox.test(data$Before, data$After, paired = TRUE, alternative = "less")

```

\vspace{1.5em}





## Kruskal Wallis Test

\vspace{1.5em}

* considered a non-parametric alternative to one way ANOVA
* used to compare differences between more than two independent groups when the dependent variable is either 
ordinal or continuous, but not normally distributed
* hypothesis tested
  * $H_{0}$: $K$ samples come from the same population 
  * $H_{1}$: not $H_{0}$

\vspace{1.5em}

```{r data13, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/non_parametric_tests/Kruskal\ Wallis\ Test.csv", header = TRUE)

```

```{r kruskal-test}

kruskal.test(aptscore ~ Group, data = data)

```

\vspace{1.5em}






## Chi-Square Test Of Association

\vspace{1.5em}

* also known as Pearson's Chi-Square Test
* used to test if there is a relationship between two categorical variables
* the two categorical variables can be nominal or ordinal 
* hypothesis tested
  * $H_{0}$: the two attributes are independent 
  * $H_{1}$: not $H_{0}$

\vspace{1.5em}

```{r data14, echo = FALSE}

data <- read.csv("../../../data/0204_statistical_inference/non_parametric_tests/chi\ square\ test\ of\ association.csv", header = TRUE)

```

```{r chi-square-test}

CrossTable(data$performance, data$source, chisq = TRUE)

```

\vspace{1.5em}




\newpage






# Summary

\vspace{1.5em}

## Table

\vspace{1.5em}

| Parametric                 | Non-Parametric                            |
|----------------------------|-------------------------------------------|
| one sample t-Test          | Wilcoxon signed rank test                 |
| independent samples t-Test | Mann-Whitney test                         |
| paired samples t-test      | Wilcoxon signed rank test for paired data |
| one way ANOVA              | Kruskal Wallis test                       |
| two way ANOVA              |                                           |
| three way ANOVA            |                                           |













\newpage

